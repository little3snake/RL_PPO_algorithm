# RL_PPO_algorithm
2nd course Coursework (a part of the Courework "Effective implementation of RL algorithms")
The usual and distributed implementations of PPO algorithm.
Testing with Bipedal Walker and Atari Breakout.
## ğŸ–¥ï¸ System Requirements
 - Windows 10/11 64-bit

 - Python 3.8+

 - NVIDIA GPU (recommended) + CUDA Toolkit

 - For the distributed version: WSL2 + Docker Desktop

## Pre-launch Requirements
 - For Bipedal Walker: swig

 - For graphs and diagrams register on Weight & Biases and log into your account in the terminal (wandb login ...)

## ğŸ—‚ï¸ Project Structure
RL_PPO_algorithm/
â”œâ”€â”€ gifs/ # gifs of the trained agents
â”‚ â”œâ”€â”€ atari_ppo_PT_post_training_final_10m.gif
â”‚ â””â”€â”€ bipedalWalker_ppo_PT_post_training_3_500_000.gif
â”œâ”€â”€ ppo_atari_breakout/ # Atari Breakout version
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ network.py
â”‚ â””â”€â”€ ppo.py
â”œâ”€â”€ ppo_bipedal_walker/ # Standard BipedalWalker version
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ network.py
â”‚ â””â”€â”€ ppo.py
â””â”€â”€ ppo_bipedal_walker_distributed/ # Distributed version (WSL2+Docker)
â”œâ”€â”€ main.py
â”œâ”€â”€ network.py
â””â”€â”€ ppo.py
